{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b7d0289e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dataset import CausalLMDataset\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from transformers import (\n",
    "    MODEL_WITH_LM_HEAD_MAPPING,\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelWithLMHead,\n",
    "    AutoTokenizer,\n",
    "    GPT2Tokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "from transformers import GPT2LMHeadModel\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "aa8d32c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>rating</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>gendered</th>\n",
       "      <th>has_gender</th>\n",
       "      <th>modified text</th>\n",
       "      <th>outputs</th>\n",
       "      <th>masked_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>I researched many breast pumps on line before ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>positive</td>\n",
       "      <td>['daughter', 'she', 'husband', 'her', 'daughter']</td>\n",
       "      <td>True</td>\n",
       "      <td>i researched many breast pumps on line before ...</td>\n",
       "      <td>[NIM] [NIM] [NIM] [NIM] [NIM] [NIM] [NIM] [NIM...</td>\n",
       "      <td>[NIM] [NIM] [NIM] [NIM] [NIM] [NIM] [NIM] [NIM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>This was a subject of much conversation when I...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>positive</td>\n",
       "      <td>['man']</td>\n",
       "      <td>True</td>\n",
       "      <td>this was a subject of much conversation when i...</td>\n",
       "      <td>[NIM] [NIM] [NIM] [NIM] [NIM] [NIM] [NIM] [NIM...</td>\n",
       "      <td>[NIM] [NIM] [NIM] [NIM] [NIM] [NIM] [NIM] [NIM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>I bought this 3 in 1 for my fiance' because he...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>positive</td>\n",
       "      <td>['he']</td>\n",
       "      <td>True</td>\n",
       "      <td>i bought this 3 in 1 for my fiance 'because [M...</td>\n",
       "      <td>[NIM] [NIM] [NIM] [NIM] [NIM] [NIM] [NIM] [NIM...</td>\n",
       "      <td>[NIM] [NIM] [NIM] [NIM] [NIM] [NIM] [NIM] [NIM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>Loved our Marathon until our 18 month old reac...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>negative</td>\n",
       "      <td>['he']</td>\n",
       "      <td>True</td>\n",
       "      <td>loved our marathon until our 18 month old reac...</td>\n",
       "      <td>[NIM] [NIM] [NIM] [NIM] [NIM] [NIM] [NIM] [NIM...</td>\n",
       "      <td>[NIM] [NIM] [NIM] [NIM] [NIM] [NIM] [NIM] [NIM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>My husband is a BMW mechanic and he drives car...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>positive</td>\n",
       "      <td>['husband', 'he', 'him']</td>\n",
       "      <td>True</td>\n",
       "      <td>my [MASK] is a bmw mechanic and [MASK] drives ...</td>\n",
       "      <td>[NIM] husband [NIM] [NIM] [NIM] [NIM] [NIM] he...</td>\n",
       "      <td>[NIM] [MAL] [NIM] [NIM] [NIM] [NIM] [NIM] [MAL...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           review_text  rating sentiment  \\\n",
       "897  I researched many breast pumps on line before ...     5.0  positive   \n",
       "262  This was a subject of much conversation when I...     5.0  positive   \n",
       "151  I bought this 3 in 1 for my fiance' because he...     5.0  positive   \n",
       "865  Loved our Marathon until our 18 month old reac...     1.0  negative   \n",
       "174  My husband is a BMW mechanic and he drives car...     5.0  positive   \n",
       "\n",
       "                                              gendered  has_gender  \\\n",
       "897  ['daughter', 'she', 'husband', 'her', 'daughter']        True   \n",
       "262                                            ['man']        True   \n",
       "151                                             ['he']        True   \n",
       "865                                             ['he']        True   \n",
       "174                           ['husband', 'he', 'him']        True   \n",
       "\n",
       "                                         modified text  \\\n",
       "897  i researched many breast pumps on line before ...   \n",
       "262  this was a subject of much conversation when i...   \n",
       "151  i bought this 3 in 1 for my fiance 'because [M...   \n",
       "865  loved our marathon until our 18 month old reac...   \n",
       "174  my [MASK] is a bmw mechanic and [MASK] drives ...   \n",
       "\n",
       "                                               outputs  \\\n",
       "897  [NIM] [NIM] [NIM] [NIM] [NIM] [NIM] [NIM] [NIM...   \n",
       "262  [NIM] [NIM] [NIM] [NIM] [NIM] [NIM] [NIM] [NIM...   \n",
       "151  [NIM] [NIM] [NIM] [NIM] [NIM] [NIM] [NIM] [NIM...   \n",
       "865  [NIM] [NIM] [NIM] [NIM] [NIM] [NIM] [NIM] [NIM...   \n",
       "174  [NIM] husband [NIM] [NIM] [NIM] [NIM] [NIM] he...   \n",
       "\n",
       "                                         masked_output  \n",
       "897  [NIM] [NIM] [NIM] [NIM] [NIM] [NIM] [NIM] [NIM...  \n",
       "262  [NIM] [NIM] [NIM] [NIM] [NIM] [NIM] [NIM] [NIM...  \n",
       "151  [NIM] [NIM] [NIM] [NIM] [NIM] [NIM] [NIM] [NIM...  \n",
       "865  [NIM] [NIM] [NIM] [NIM] [NIM] [NIM] [NIM] [NIM...  \n",
       "174  [NIM] [MAL] [NIM] [NIM] [NIM] [NIM] [NIM] [MAL...  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data\n",
    "df = pd.read_csv('data/cleaned_data_labeled.csv', index_col=0)\n",
    "\n",
    "# train-test split\n",
    "train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b987183d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"i was blown away by the wonderful acting of juliet lewis and giovanni ribisi who played the retarded young adults. this was a very compassionate film about the realities of living with a disability. disabled people desire and deserve to have the same opportunities as everyone else. it was wonderful to see the love carla 's siblings and parents had for her. there were 2 reasons i didn't give this movie 5 stars. one was that diane keaton's character of the overly controlling mother was annoying. obviously she was supposed to be annoying, but i think she overacted a bit and she needed better lines, she seemed to say the same stuff over and over again. another reason i didn't love this film was because hollywood once again compels right wing nut jobs to write reviews that portray themselves as normal and good. sorry, hollywood, but there will always be those of us who still believe god's word to be true. being a right wing nut job isn't considered normal or ok in the bible. god never created it. look it up. you are not helping people who struggle with it to be set free from that addiction and lonely lifestyle.\"}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read data\n",
    "df = pd.read_csv('data/cleaned_data_labeled.csv', index_col=0)\n",
    "\n",
    "# train-test split\n",
    "train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\n",
    "    'distilgpt2',\n",
    "    pad_token = '<|endoftext|>'  # original tokenizer does not have pad token\n",
    ")\n",
    "\n",
    "# datasets\n",
    "train_dataset = CausalLMDataset(\n",
    "    df = train_df,\n",
    "    text_col = 'review_text'\n",
    ")\n",
    "val_dataset = CausalLMDataset(\n",
    "    df = val_df,\n",
    "    text_col = 'review_text'\n",
    ")\n",
    "test_dataset = CausalLMDataset(\n",
    "    df = test_df,\n",
    "    text_col = 'review_text'\n",
    ")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9f5ff72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalLanguageModelingCollate:\n",
    "    \n",
    "    '''\n",
    "    collate_fn in dataloader is used for post processing on a single batch. Like __getitem__ in dataset class\n",
    "    is used on single example\n",
    "    '''\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        tokenizer,\n",
    "        _tok_return_tensors = 'pt',\n",
    "        _tok_max_length = None,\n",
    "        _tok_truncation = False,\n",
    "        _tok_padding = False\n",
    "    ):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self._tok_return_tensors = _tok_return_tensors\n",
    "        self._tok_max_length = _tok_max_length\n",
    "        self._tok_truncation = _tok_truncation\n",
    "        self._tok_padding = _tok_padding\n",
    "        \n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        '''\n",
    "        __call__: a default method\n",
    "        First the obj is created using MyCollate(pad_idx) in data loader\n",
    "        Then if obj(batch) is called -> __call__ runs by default\n",
    "        \n",
    "        Overwrites `input_ids` element of each input in `batch` with a partially-masked version.\n",
    "        '''\n",
    "        \n",
    "        # grab text\n",
    "        batch_texts = [example['text'] for example in batch]\n",
    "\n",
    "        # tokenize texts with the tokenizer: ['quick', 'brown', 'fox'] -> [12, 2, 9, 0]\n",
    "        # ['jumps' 'over', 'the'] -> [9, 12, 45, 0]\n",
    "        # ['the' lazy', 'dog' '.'] -> [123,456,789,098]\n",
    "        tokenized_batch = self.tokenizer(\n",
    "            batch_texts,\n",
    "            return_tensors = self._tok_return_tensors, \n",
    "            max_length = self._tok_max_length, \n",
    "            truncation = self._tok_truncation, \n",
    "            padding = self._tok_padding\n",
    "        )\n",
    "\n",
    "        # duplicate inputs as targets\n",
    "        tokenized_batch['labels'] = tokenized_batch.input_ids.detach().clone()\n",
    "\n",
    "        return tokenized_batch\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d55e361b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from collate_fns import DialogCollate\n",
    "\n",
    "# collate object\n",
    "collate_fn = CausalLanguageModelingCollate(\n",
    "    tokenizer = tokenizer,\n",
    "    _tok_return_tensors = 'pt',\n",
    "    _tok_max_length = 512,\n",
    "    _tok_truncation = True,\n",
    "    _tok_padding = 'longest'\n",
    ")\n",
    "\n",
    "# train dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = 2,\n",
    "    shuffle = True,\n",
    "    collate_fn = collate_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size = 2,\n",
    "    shuffle = False,\n",
    "    collate_fn = collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size = 2,\n",
    "    shuffle = False,\n",
    "    collate_fn = collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5859222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<|endoftext|>',\n",
       " 'eos_token': '<|endoftext|>',\n",
       " 'unk_token': '<|endoftext|>',\n",
       " 'pad_token': '<|endoftext|>'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a10f2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids of example 0 in batch:\n",
      " tensor([   72,   655,   836,   470,   760,   810,   284,   923,   764,   612,\n",
      "          655,  1595,   470,  1283,   284,   307,   257,   835,   284,  6901,\n",
      "          262,  4467,   764,   345,   481,   655,   423,   284,  1949,   606,\n",
      "          319,   284,  1975,   340,   764,   616,  4780,   837,  2802,   837,\n",
      "         5229,   290,  1312,   477, 21192,   416,   428,  1720,   764,  2453,\n",
      "          645, 21436,  3508,   764, 10966,  2644,   645,   837,   475,   262,\n",
      "         4467,   481,  6611,   345,  1497, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256])\n",
      "target_ids of example 0 in batch:\n",
      " tensor([   72,   655,   836,   470,   760,   810,   284,   923,   764,   612,\n",
      "          655,  1595,   470,  1283,   284,   307,   257,   835,   284,  6901,\n",
      "          262,  4467,   764,   345,   481,   655,   423,   284,  1949,   606,\n",
      "          319,   284,  1975,   340,   764,   616,  4780,   837,  2802,   837,\n",
      "         5229,   290,  1312,   477, 21192,   416,   428,  1720,   764,  2453,\n",
      "          645, 21436,  3508,   764, 10966,  2644,   645,   837,   475,   262,\n",
      "         4467,   481,  6611,   345,  1497, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256])\n",
      "attention_masks of example 0 in batch:\n",
      " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"i just don't know where to start. there just doesn't seem to be a way to describe the comfort. you will just have to try them on to believe it. my neighbor, mother, husband and i all swear by this product. accept no substitutions. attractive... no, but the comfort will blow you away<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example usage\n",
    "for batch in val_loader:\n",
    "    break\n",
    "\n",
    "example_id = 0\n",
    "print(f'input_ids of example {example_id} in batch:\\n',       batch['input_ids'][example_id])\n",
    "print(f'target_ids of example {example_id} in batch:\\n',      batch['labels'][example_id])\n",
    "print(f'attention_masks of example {example_id} in batch:\\n', batch['attention_mask'][example_id])\n",
    "\n",
    "tokenizer.decode(batch['input_ids'][example_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32883dd4",
   "metadata": {},
   "source": [
    "Explanation: For GPT-2 models, the `sos`, `bos`, `unk`, and `pad` are all represetned by the same token: `'<|endoftext|>'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce304f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# infinite positive sample loader\n",
    "\n",
    "class InfiniteDataGen:\n",
    "    def __init__(self, dataloader):\n",
    "        self.dataloader = dataloader\n",
    "        self.epoch_count = 0\n",
    "    \n",
    "    def generate(self): \n",
    "        while True:\n",
    "            for batch in self.dataloader:\n",
    "                yield batch\n",
    "            self.epoch_count += 1\n",
    "            \n",
    "\n",
    "def create_json(target_dir, filename):\n",
    "    json_as_string = json.dumps({}, indent=4, sort_keys=False)\n",
    "    \n",
    "    if not os.path.exists(target_dir): \n",
    "        os.makedirs(target_dir)\n",
    "    \n",
    "    with open(os.path.join(target_dir, filename), \"w\") as outfile:\n",
    "        outfile.write(json_as_string)\n",
    "            \n",
    "\n",
    "def update_json(target_dir, filename, dict_to_save={}):\n",
    "    \n",
    "    f = open(os.path.join(target_dir, filename), \"r\")\n",
    "    old_dict = json.loads(f.read())\n",
    "    old_dict.update(dict_to_save)\n",
    "\n",
    "    json_as_string = json.dumps(old_dict, indent=4, sort_keys=False)\n",
    "    with open(os.path.join(target_dir, filename), \"w\") as outfile:\n",
    "        outfile.write(json_as_string)\n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "861b9f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_json(\n",
    "#     target_dir = CONFIG['LOGGING_DIR'],\n",
    "#     filename = CONFIG['EXPERIMENT_NAME'] + '.json'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53f71725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update_json(\n",
    "#     target_dir = CONFIG['LOGGING_DIR'],\n",
    "#     filename = CONFIG['EXPERIMENT_NAME'] + '.json',\n",
    "#     dict_to_save = {0: {'pos_loss': 0.1, 'neg_loss': 0.5}}\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "920b8f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# infinite_positive_data_obj = InfiniteDataGen(notok_loader)\n",
    "# infinite_positive_data_gen = infinite_positive_data_obj.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce2f2310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_batches_processed = 0\n",
    "\n",
    "# for _ in range(5000):\n",
    "    \n",
    "#     batch = next(infinite_positive_data_gen)\n",
    "    \n",
    "#     if num_batches_processed % 10 == 0:\n",
    "#         print(f'\\rNumber of batches processed: {num_batches_processed}, epoch {infinite_positive_data_obj.epoch_count}', end='', flush=True)\n",
    "        \n",
    "        \n",
    "#     num_batches_processed += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4714bff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'EXPERIMENT_NAME': 'initial',\n",
    "    \n",
    "    'START_ITER': 0,\n",
    "    'TRAIN_ITERS': 20,\n",
    "    \n",
    "    'LOGGING_DIR': './logs/',\n",
    "    'MODEL_SAVE_DIR': './checkpoints/',\n",
    "    \n",
    "    'HUGGINGFACE_MODEL_NAME': 'distilgpt2',\n",
    "    'POS_LR': 0.001,\n",
    "    'NEG_LR': 0.001,\n",
    "    'UPDATES_PER_BATCH': 20,\n",
    "    \n",
    "    'EXAMPLE_WEIGHT_MODE': 'decay',\n",
    "    'EXAMPLE_WEIGHT_CARE_MODE': 'sample_avg',\n",
    "    'EXAMPLE_WEIGHT_REJECTION_THRESHOLD': -7.0,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb643e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import GPT2LMHeadModel\n",
    "# from torch.optim import AdamW\n",
    "\n",
    "# torch.manual_seed(0)\n",
    "# DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# MODEL_SAVE_DIR = 'checkpoints/test/'\n",
    "\n",
    "# if not os.path.exists(MODEL_SAVE_DIR):\n",
    "#     os.makedirs(MODEL_SAVE_DIR)\n",
    "    \n",
    "# model = GPT2LMHeadModel.from_pretrained(\"microsoft/DialoGPT-medium\") \n",
    "# model.to(DEVICE)\n",
    "# model.train()\n",
    "# print('Number of model parameters:', count_parameters(model))\n",
    "\n",
    "# # optimizer\n",
    "# from torch.optim import AdamW\n",
    "# optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "93db59bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "from torch.optim import AdamW\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class CausalLMWrapper:\n",
    "\n",
    "    \n",
    "    def __init__(self, model_name, opt_lr, tokenizer, device):\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=opt_lr)\n",
    "        self.opt_lr = opt_lr\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "        \n",
    "    def reset_optimizers(self, opt_lr=None):\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=opt_lr if opt_lr is not None else self.opt_lr)\n",
    "    \n",
    "    \n",
    "    def maximum_likelihood_step(self, batch):\n",
    "        '''\n",
    "        Performs a standard (min. negative log likelihood / max. log likelihood)\n",
    "        gradient update on `self.model` using `batch`.\n",
    "        '''\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # max sequence length\n",
    "        seq_len = batch['input_ids'].shape[1]\n",
    "        \n",
    "        # each tensor to device\n",
    "        input_ids = batch['input_ids'].to(self.device)\n",
    "        labels    = batch['labels'].to(self.device)\n",
    "        # print(input_ids.shape, position_ids.shape, token_type_ids.shape, target_ids.shape, attention_mask.shape)\n",
    "\n",
    "        # get model outputs\n",
    "        outputs = self.model(\n",
    "            input_ids = input_ids,\n",
    "            labels = labels\n",
    "        )\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # consider gradient clipping here before updating\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return  {\n",
    "            'loss': loss\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def train_maximum_likelihood_one_epoch(train_loader):\n",
    "        '''\n",
    "        Performs one epoch of maximum likelihood training.\n",
    "        '''\n",
    "        batch_wise_losses = []\n",
    "        for batch_id, batch in enumerate(train_loader):\n",
    "\n",
    "            print(f'\\rProcessing neg batch {batch_id} of {len(train_loader)}', end='', flush=True)\n",
    "\n",
    "            self..optimizer.zero_grad()\n",
    "            results = self..maximum_likelihood_step(batch)\n",
    "            batch_wise_losses.append(results['loss'].item())\n",
    "\n",
    "        return {\n",
    "            'batch_wise_losses': batch_wise_losses,\n",
    "            'average_loss': np.array(batch_wise_losses).mean(),\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def forward_logprobs_of_batch(self, batch):\n",
    "        '''\n",
    "        Returns the forward pass results of `batch` in their (masked) log-prob form.\n",
    "        Masking is performed on the basis of `ignore_index` (TODO: change \n",
    "        `ignore_index` from a magic number to a proper parameter); all tokens with\n",
    "        value `ignore_index` will contribute a log probability of zero.\n",
    "        \n",
    "        Returns a tensor of shape (batch_size, seq_len)\n",
    "        '''\n",
    "\n",
    "        # max sequence length\n",
    "        seq_len = batch['input_ids'].shape[1]\n",
    "        \n",
    "        # each tensor to device\n",
    "        input_ids = batch['input_ids'].to(self.device)\n",
    "        labels    = batch['labels'].to(self.device)\n",
    "        # print(input_ids.shape, position_ids.shape, token_type_ids.shape, target_ids.shape, attention_mask.shape)\n",
    "\n",
    "        # get model outputs\n",
    "        outputs = self.model(\n",
    "            input_ids = input_ids,\n",
    "            labels = labels\n",
    "        )\n",
    "        \n",
    "        # calculate the masked logprobs\n",
    "        masked_logprobs_of_true_labels, ignore_index_mask = \\\n",
    "        CausalLMWrapper._calculate_masked_logprobs(\n",
    "            logits = outputs.logits,  # (batch_size, seq_len)\n",
    "            labels = labels,  # (batch_size, seq_len, vocab_size)\n",
    "            ignore_index = -100,  # TODO: remove this magic number\n",
    "        )\n",
    "        \n",
    "        return  {\n",
    "            'masked_logprobs': masked_logprobs_of_true_labels,  # (batch_size, seq_len)\n",
    "            'ignore_index_mask': ignore_index_mask  # (batch_size, seq_len)\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def nll_on_dataset(self, dataloader):\n",
    "        '''\n",
    "        Returns the average negative log-likelihood on the examples in a `dataloader`\n",
    "        '''\n",
    "        batch_wise_losses = []\n",
    "        len_loader = len(dataloader)\n",
    "        for batch_id, batch in enumerate(dataloader):\n",
    "\n",
    "            if batch_id % 10 == 0:\n",
    "                print(f'\\rEvaluating batch: {batch_id} of {len_loader}', end='', flush=True)\n",
    "\n",
    "            # calculate negative log likelihood loss\n",
    "            logprobs_result = self.forward_logprobs_of_batch(batch)\n",
    "            example_wise_loss = torch.sum(logprobs_result['masked_logprobs'], dim = 1)\n",
    "            loss = -example_wise_loss.sum() / logprobs_result['ignore_index_mask'].sum()\n",
    "            batch_wise_losses.append(loss.item())\n",
    "            \n",
    "        return np.array(batch_wise_losses).mean()\n",
    "    \n",
    "    \n",
    "    def batch_generate_samples_unconditional(self, num_samples, out_file, batch_size=8, max_new_tokens=100):\n",
    "        '''\n",
    "        Sample `num_samples` samples from the model, `batch_size` samples at a time.\n",
    "        Sampling is done unconditionally (no conditioning prompt) and independently of each other.\n",
    "        Saves the samples into `out_file`.\n",
    "        '''\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        with open(out_file, 'w') as fout:\n",
    "            \n",
    "            num_generated_samples = 0\n",
    "            while num_generated_samples < num_samples:\n",
    "\n",
    "                print(f'\\rGenerated {num_generated_samples} of {num_samples}', end='', flush=True)\n",
    "\n",
    "                outputs = self.model.generate(\n",
    "                    max_new_tokens = max_new_tokens,\n",
    "                    pad_token_id = self.tokenizer.eos_token_id,\n",
    "                    do_sample = True,  # do_sample = True; otherwise all questions will be identical\n",
    "                    top_p = 0.95,      # nucleus sampling\n",
    "                    top_k = 0,         # deactivate top-k words sampling\n",
    "                    num_return_sequences = batch_size\n",
    "                ).cpu().tolist()\n",
    "                num_generated_samples += batch_size\n",
    "                \n",
    "                outputs = [\n",
    "                    [str(token) for token in token_seq if token != self.tokenizer.eos_token_id]\n",
    "                    for token_seq in outputs\n",
    "                ]\n",
    "                \n",
    "                outputs_tokens_joined = [' '.join(token_seq) for token_seq in outputs]\n",
    "                \n",
    "                \n",
    "                # write the tokens to the output file\n",
    "                fout.write('\\n'.join(outputs_tokens_joined))\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def _onehot_maskgen(sz, idx):\n",
    "        msk = torch.BoolTensor(sz)\n",
    "        msk.fill_(False)\n",
    "        msk[torch.LongTensor(range(sz[0])), idx.cpu()] = True\n",
    "        if idx.is_cuda == True:\n",
    "            msk = msk.cuda()\n",
    "        return Variable(msk)\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def _calculate_masked_logprobs(logits, labels, ignore_index):\n",
    "        batch_size = logits.shape[0]\n",
    "        seq_len    = logits.shape[1]\n",
    "        vocab_size = logits.shape[-1]\n",
    "\n",
    "        # shift labels to align with logits\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "        # and truncate logits to make shapes the same\n",
    "        trunc_logits = logits[..., :-1, :].contiguous()\n",
    "\n",
    "        # flatten logits and labels\n",
    "        flat_logits = trunc_logits.view(-1, trunc_logits.shape[-1])  # (batch_size*seq_len, vocab_size)\n",
    "        flat_labels = shift_labels.view(-1)  # (batch_size*seq_len)\n",
    "\n",
    "        # calculate the log probabilities\n",
    "        flat_logprobs = torch.nn.functional.log_softmax(flat_logits, dim = -1)\n",
    "        \n",
    "        # for each example, grab the log probability corresponding to the true label\n",
    "        onehot_labels = CausalLMWrapper._onehot_maskgen(flat_logprobs.size(), flat_labels.data).to(torch.bool)\n",
    "        logprobs_of_true_labels = torch.masked_select(flat_logprobs, onehot_labels).view(batch_size, -1)  # (batch_size, seq_len)\n",
    "\n",
    "        # mask out locations with value `ignore_index`\n",
    "        ignore_index_mask = ~(shift_labels == ignore_index)  # 0 when value == ignore_index, 1 otherwise\n",
    "        return logprobs_of_true_labels * ignore_index_mask, ignore_index_mask\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def _calculate_positive_phase_loss(logits, labels, ignore_index):\n",
    "        '''\n",
    "        Returns the (unweighted) negative log likelihood of a batch.\n",
    "        '''\n",
    "        \n",
    "        # get masked logprobs\n",
    "        masked_logprobs_of_true_labels, ignore_index_mask = \\\n",
    "        CausalLMWrapper._calculate_masked_logprobs(\n",
    "            logits, labels, ignore_index\n",
    "        )\n",
    "        \n",
    "        # loss\n",
    "        example_wise_loss = torch.sum(masked_logprobs_of_true_labels, dim = 1)\n",
    "        loss = - example_wise_loss.sum() / ignore_index_mask.sum()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "00b658a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "\n",
    "# preparing logging\n",
    "create_json(\n",
    "    target_dir = CONFIG['LOGGING_DIR'],\n",
    "    filename = CONFIG['EXPERIMENT_NAME'] + '.json'\n",
    ")\n",
    "    \n",
    "# preparing model checkpointing\n",
    "if not os.path.exists(CONFIG['MODEL_SAVE_DIR']): \n",
    "    os.makedirs(CONFIG['MODEL_SAVE_DIR'])\n",
    "\n",
    "# model\n",
    "model_wrapper = CausalLMWrapper(\n",
    "    model_name = CONFIG['HUGGINGFACE_MODEL_NAME'],\n",
    "    opt_lr = 0.001,\n",
    "    tokenizer = tokenizer,\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    ")\n",
    "\n",
    "# collate object\n",
    "collate_fn = CausalLanguageModelingCollate(\n",
    "    tokenizer = tokenizer,\n",
    "    _tok_return_tensors = 'pt',\n",
    "    _tok_max_length = 512,\n",
    "    _tok_truncation = True,\n",
    "    _tok_padding = 'longest'\n",
    ")\n",
    "\n",
    "# train dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = 2,\n",
    "    shuffle = True,\n",
    "    collate_fn = collate_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size = 2,\n",
    "    shuffle = False,\n",
    "    collate_fn = collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size = 2,\n",
    "    shuffle = False,\n",
    "    collate_fn = collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f239c31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 24 of 32"
     ]
    }
   ],
   "source": [
    "# example of generating text\n",
    "samples = model_wrapper.batch_generate_samples_unconditional(\n",
    "    num_samples = 32, \n",
    "    out_file = 'gen_data.csv',\n",
    "    batch_size = 8, \n",
    "    max_new_tokens = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ba66e066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"BloCK makes some interesting predictions that are not insurmountable. As such, after wondering what's going on on, you can really just drop it straight away.\\n\\n\\nThe Mednecine Perspective\\nA lot of these figures might be wondering if the fact that we now hear Akali was just a holdout and here they come from.\\nRemember that Akali works pretty well in r/s (the girl like that). We already see her doing strong substandard things like\"}\n",
      "input_ids of example 0 in batch:\n",
      " tensor([   44,  1025,   722,   290, 30481, 15595, 32329,   357,  6144,    72,\n",
      "            8,  6822,   503,   674, 11808,   319,   788,    12, 35352,  5436,\n",
      "         5882,  3841,   543,  6774,   345,  2279,   345,   761,   284,   307,\n",
      "         3910,   286,    13, 18067, 31026, 17056,   870, 36109,   656, 18892,\n",
      "        12044,  1496,  4217,  4128, 17267,   625,   422,   257, 12744,  4217,\n",
      "         1471, 15595,  4217,  5972,  2667,  8554, 21252,    82,   770, 18749,\n",
      "          481,  1037,   284,  4545,   262, 46264,   835,   284,   651,  2067,\n",
      "          319,   257,  4217,    11,  1231,  1016,   477,   262,   835,   284,\n",
      "          257, 12744,  4217,   399,  6351,  1352,  9447,    33,  4572,    13,\n",
      "          632,   318,  2592,  7613,   611,   262,  1541,    12,   259,  1247,\n",
      "          417, 21707,   318,   287,   257,  1597,  1413,   351, 36763,  5018,\n",
      "          960,   272,  4260,  8747,   286,   262, 11565,  7965,   960,  4360,\n",
      "        13258, 28697,  7830,  6848,   326,   339,   338,   407,  2111,   284,\n",
      "          787,   257,   366,    75, 12582,  1637,   553,   257, 12046,   326,\n",
      "         4193,  2245,   257,  2168,   286,  8381, 11754, 28449,    13,   628,\n",
      "          628,   198,   198,    47,   417, 21707,  1139,   326,   339,  1422,\n",
      "          470,   772,   892,   286, 13258, 12155,   878,   339,  1364,   262,\n",
      "         1181,   286, 11565,   287,  7795,    11,  1141,   543,   339,   468,\n",
      "          973, 13258, 28697, 49090,   355,   257,   366,  4102,  2357,     1,\n",
      "          284,   787,   257,  3877,   326, 28067,   290])\n",
      "target_ids of example 0 in batch:\n",
      " tensor([   44,  1025,   722,   290, 30481, 15595, 32329,   357,  6144,    72,\n",
      "            8,  6822,   503,   674, 11808,   319,   788,    12, 35352,  5436,\n",
      "         5882,  3841,   543,  6774,   345,  2279,   345,   761,   284,   307,\n",
      "         3910,   286,    13, 18067, 31026, 17056,   870, 36109,   656, 18892,\n",
      "        12044,  1496,  4217,  4128, 17267,   625,   422,   257, 12744,  4217,\n",
      "         1471, 15595,  4217,  5972,  2667,  8554, 21252,    82,   770, 18749,\n",
      "          481,  1037,   284,  4545,   262, 46264,   835,   284,   651,  2067,\n",
      "          319,   257,  4217,    11,  1231,  1016,   477,   262,   835,   284,\n",
      "          257, 12744,  4217,   399,  6351,  1352,  9447,    33,  4572,    13,\n",
      "          632,   318,  2592,  7613,   611,   262,  1541,    12,   259,  1247,\n",
      "          417, 21707,   318,   287,   257,  1597,  1413,   351, 36763,  5018,\n",
      "          960,   272,  4260,  8747,   286,   262, 11565,  7965,   960,  4360,\n",
      "        13258, 28697,  7830,  6848,   326,   339,   338,   407,  2111,   284,\n",
      "          787,   257,   366,    75, 12582,  1637,   553,   257, 12046,   326,\n",
      "         4193,  2245,   257,  2168,   286,  8381, 11754, 28449,    13,   628,\n",
      "          628,   198,   198,    47,   417, 21707,  1139,   326,   339,  1422,\n",
      "          470,   772,   892,   286, 13258, 12155,   878,   339,  1364,   262,\n",
      "         1181,   286, 11565,   287,  7795,    11,  1141,   543,   339,   468,\n",
      "          973, 13258, 28697, 49090,   355,   257,   366,  4102,  2357,     1,\n",
      "          284,   787,   257,  3877,   326, 28067,   290])\n",
      "attention_masks of example 0 in batch:\n",
      " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Moothing and Creating Virtual Models (NNi) Check out our tutorial on then-editor Max Longford which brings you everything you need to be aware of. Getting Started Configuring Components into Bootstrapped PC Direct Import over from a Target PC Or Virtual PC Logging Using SSDs This FAQ will help to teach the quickest way to get started on a PC, without going all the way to a Target PC Nominator SMB machine. It is especially helpful if the already-in centelosi is in a business camp with Deutsche Bank—an alleged violation of the Missouri Constitution—but Pope Benedict repeatedly admitted that he\\'s not trying to make a \"lazy money,\" a stance that helped stop a series of controversial banking scandals.\\n\\n\\n\\n\\n\\nPelosi says that he didn\\'t even think of Pope Francis before he left the state of Missouri in 1998, during which he has used Pope Benedict XVI as a \"marketer\" to make a charge that gays and'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# datasets and dataloaders from the generated data\n",
    "generated_df = pd.read_csv('gen_data.csv', header=None)\n",
    "token_list = generated_df[0]\n",
    "\n",
    "# dataset from tokens\n",
    "generated_dataset = CausalLMDataset(\n",
    "    token_list = token_list,\n",
    "    tokenizer = tokenizer,\n",
    ")\n",
    "print(generated_dataset[0])\n",
    "\n",
    "# collate object\n",
    "generated_collate_fn = CausalLanguageModelingCollate(\n",
    "    tokenizer = tokenizer,\n",
    "    _tok_return_tensors = 'pt',\n",
    "    _tok_max_length = 512,\n",
    "    _tok_truncation = True,\n",
    "    _tok_padding = 'longest'\n",
    ")\n",
    "\n",
    "# train dataloaders\n",
    "generated_loader = DataLoader(\n",
    "    generated_dataset,\n",
    "    batch_size = 2,\n",
    "    shuffle = True,\n",
    "    collate_fn = generated_collate_fn\n",
    ")\n",
    "\n",
    "# example usage\n",
    "for batch in generated_loader:\n",
    "    break\n",
    "\n",
    "example_id = 0\n",
    "print(f'input_ids of example {example_id} in batch:\\n',       batch['input_ids'][example_id])\n",
    "print(f'target_ids of example {example_id} in batch:\\n',      batch['labels'][example_id])\n",
    "print(f'attention_masks of example {example_id} in batch:\\n', batch['attention_mask'][example_id])\n",
    "\n",
    "tokenizer.decode(batch['input_ids'][example_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fde3d469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating batch: 3860 of 3863\n",
      "NLL of model on train dataset: 6.045377352371631\n"
     ]
    }
   ],
   "source": [
    "# evaluate before training\n",
    "mean_nll = model_wrapper.nll_on_dataset(train_loader)\n",
    "print('\\nNLL of model on train dataset:', mean_nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "209dcccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing neg batch 36 of 3863"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9736/1687008320.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# perform one \"negative epoch\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     res = train_maximum_likelihood_one_epoch(\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mtrain_loader\u001b[0m      \u001b[1;33m=\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mmodel_wrapper\u001b[0m     \u001b[1;33m=\u001b[0m \u001b[0mmodel_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9736/3313646530.py\u001b[0m in \u001b[0;36mtrain_maximum_likelihood_one_epoch\u001b[1;34m(train_loader, model_wrapper)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mmodel_wrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_wrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximum_likelihood_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mbatch_wise_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     return {\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_wise_losses = []\n",
    "epoch_wise_losses = []\n",
    "for it in range(CONFIG['START_ITER'], CONFIG['START_ITER'] + CONFIG['TRAIN_ITERS']):  \n",
    "\n",
    "    # restart the optimizer at each pos-neg iteration\n",
    "    model_wrapper.reset_optimizers()\n",
    "    \n",
    "    # train on one epoch\n",
    "    res = model_wrapper.train_maximum_likelihood_one_epoch(\n",
    "        train_loader      = train_loader\n",
    "    )\n",
    "    \n",
    "    epoch_wise_losses.append(res['average_loss'])\n",
    "    batch_wise_losses.extend(res['batch_wise_losses'])\n",
    "    \n",
    "    # evaluate validation performance\n",
    "    # loss_test = evaluate(...)\n",
    "    \n",
    "# perform final validation performance\n",
    "# loss_test_final = evaluate(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95b6f86",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5140b1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dataset import CausalLMDataset\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from transformers import (\n",
    "    MODEL_WITH_LM_HEAD_MAPPING,\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelWithLMHead,\n",
    "    AutoTokenizer,\n",
    "    GPT2Tokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "from transformers import GPT2LMHeadModel\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0a48b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\n",
    "    'distilgpt2',\n",
    "    pad_token = '<|endoftext|>'  # original tokenizer does not have pad token\n",
    ")\n",
    "\n",
    "####################################################################################################\n",
    "# define networks\n",
    "generator = CausalLMWrapper(\n",
    "    model_name = CONFIG['HUGGINGFACE_MODEL_NAME'],\n",
    "    opt_lr = 0.001,\n",
    "    tokenizer = tokenizer,\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    ")\n",
    "# discriminator = ...\n",
    "\n",
    "####################################################################################################\n",
    "# get downstream corpus as \"true data\" distribution\n",
    "df = pd.read_csv('data/cleaned_data_labeled.csv', index_col=0)\n",
    "\n",
    "# true data dataset\n",
    "positive_dataset = CausalLMDataset(\n",
    "    df = df,\n",
    "    text_col = 'review_text'\n",
    ")\n",
    "\n",
    "# collate object\n",
    "positive_collate_fn = CausalLanguageModelingCollate(\n",
    "    tokenizer = tokenizer,\n",
    "    _tok_return_tensors = 'pt',\n",
    "    _tok_max_length = 512,\n",
    "    _tok_truncation = True,\n",
    "    _tok_padding = 'longest'\n",
    ")\n",
    "\n",
    "# positive data dataloaders\n",
    "positive_loader = DataLoader(\n",
    "    positive_dataset,\n",
    "    batch_size = 2,\n",
    "    shuffle = True,\n",
    "    collate_fn = positive_collate_fn\n",
    ")\n",
    "\n",
    "####################################################################################################\n",
    "# pretrain generator using MLE\n",
    "batch_wise_losses = []\n",
    "epoch_wise_losses = []\n",
    "for it in range(CONFIG['START_ITER'], CONFIG['START_ITER'] + CONFIG['TRAIN_ITERS']):  \n",
    "    generator.reset_optimizers()\n",
    "    res = generator.train_maximum_likelihood_one_epoch(train_loader = train_loader)\n",
    "    # mean_nll = generator.nll_on_dataset(train_loader)\n",
    "    epoch_wise_losses.append(res['average_loss'])\n",
    "    batch_wise_losses.extend(res['batch_wise_losses'])\n",
    "    \n",
    "####################################################################################################\n",
    "# pretrain discriminator\n",
    "batch_wise_losses = []\n",
    "epoch_wise_losses = []\n",
    "for it in range(CONFIG['START_ITER'], CONFIG['START_ITER'] + CONFIG['TRAIN_ITERS']):  \n",
    "    discriminator.reset_optimizers()\n",
    "    res = discriminator.train_maximum_likelihood_one_epoch(train_loader = train_loader)\n",
    "    # mean_nll = discriminator.nll_on_dataset(train_loader)\n",
    "    epoch_wise_losses.append(res['average_loss'])\n",
    "    batch_wise_losses.extend(res['batch_wise_losses'])\n",
    "    \n",
    "####################################################################################################\n",
    "# adversarial training\n",
    "\n",
    "rollout = Rollout(generator, 0.8)\n",
    "# disc_loss_obj1 = Disc1()\n",
    "# disc_loss_obj2 = Disc2()\n",
    "# disc_loss_obj3 = Disc3()\n",
    "\n",
    "print('#####################################################')\n",
    "print('Start Adeversarial Training...\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced_deep_learning",
   "language": "python",
   "name": "advanced_deep_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
